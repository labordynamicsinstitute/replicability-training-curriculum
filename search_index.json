[["index.html", "TRAINING For Reproducibility Verification Chapter 1 Introduction 1.1 Tentative Agenda 1.2 How to read this document", " TRAINING For Reproducibility Verification Lars Vilhuber Meredith Welch David Wasser 2020-08-21 Chapter 1 Introduction This document is meant to guide the training of students in assessing the reproducibility of articles in the social sciences, and in particular in economics. 1.1 Tentative Agenda Items that are bolded are joint with the Computational Tools for Social Scientists Workshop (for graduate students). Items in italics are optional. Time Aug 24 Aug 25 Aug 26 Aug 27 Aug 28 1:00 (1:30) Welcome to LDI Lab Coffee hour: What does the AEA Data and Code Availability policy imply for an economist's research? 2:00 Intro to: Reproducible practices Breakout groups: debugging software and accounts (reserved for working on test articles) (reserved for working on test articles) (reserved for working on test articles) 3:00 Reproducible practices, data citation A prototypical replication report Small group peer mentoring Small group peer mentoring Follow-up on test articles (whole group) 4:00 A Markdown reproducible report with Stata and R A walkthrough of the workflow for unpublished articles (reserved for working on test articles) (reserved for working on test articles) 5:00 What will you be doing in the Lab Basics and advanced version control (reserved for working on test articles) (reserved for working on test articles) Training will occur virtually, through a combination of required self-study and live Zoom meetings. The live part of the training will take place Aug 24, Aug 25, and Aug 28, 2020. If your application to the LDI Replication Lab was accepted, you will be receiving a calendar invite with the Zoom information soon. All the remaining information here is open to anybody. Content is . 1.2 How to read this document If you are a casual reader of this document, start with the pre-requisites. If you are a student participating in our training, then start with the pre-training tasks. Read the rest sequentially. "],["pre-requisites.html", "Chapter 2 Pre-requisites", " Chapter 2 Pre-requisites Most students with some prior experience with statistical software can effectively reproduce and assess articles. In economics, most articles use either Stata or Matlab. Software in economics Students should be comfortable working on a variety of computers, including their own, and be flexible with respect to the location of computing. If you don't know what that means, you'll find out during training! "],["pre-training.html", "Chapter 3 Pre-training tasks", " Chapter 3 Pre-training tasks We ask that trainees accomplish a few tasks prior to the first training session. Please do the following: The training is virtual, using video, and we frequently meet virtually. Please review our Video Etiquette rules (they are useful beyond our group as well) View my recent talk on the background of the lab), including what we do, and why we do it. Review our Privacy Policy, where you will recognize how we handle your privacy, and the privacy of authors. Go through our Setup Checklist and install necessary software "],["background.html", "Chapter 4 Background 4.1 Activities of the LDI Replication Lab 4.2 Learning goals", " Chapter 4 Background On July 16, 2019, the AEA announced an updated &quot;Data and Code Availability Policy&quot; (American Economic Association 2019; American Economic Association 2020). Henceforth, replication materials were to be made available to the AEA prior to acceptance - previously, it was prior to publication, but after acceptance. Computer code should be provided for all stages of the data cleaning and data analysis (code for the data cleaning portion was previously optional). Raw data must be uniformly made available, when permissions allow (also for author-collected survey data, data from experiments). For restricted-access or proprietary data, to the extent permissible, the data must be made available to the AEA Data Editor for verification, even if the data cannot be published by the author.1 Enforcement of an existing but unenforced data citation requirement as per AEA Citation Guidelines. We also test licenses, access restrictions, and sometimes question the ability of authors to publish the data. We have had cases both ways: data provided after initial refusal, and data rejected by us because the license did not allow distribution. For now, we also check for obvious personally identifiable information (PII). However, the ultimate responsibility lies with authors. All data and code must be available in a &quot;trusted repository,&quot; which in most cases means the AEA's Data and Code Repository at openICPSR, for better transparency and findability. ZIP files are no longer accepted as supplementary packages, and we check that. Supplements are tagged with JEL codes, other keywords (e.g., Current Population Survey'' orbehavioral study''), and optionally with methodological information (time period, geographic region, survey method used). Each deposit gets its own DOI - a permanent unique identifier. Deposits can be found through various search engines, such as the native search engine on openICPSR, through Google Data Search or through DOI registries such as DataCite. To implement all this, we built a system using Jira, and connect to it from ScholarOne (system used for manuscript submission) and openICPSR. 4.1 Activities of the LDI Replication Lab The LDI Replication Lab conducts reproducibility checks in two way. 4.1.1 Pre-Publication Evaluation of Reproducibility and Quality of Supplemental Materials The Lab performs pre-publication evaluation for the American Economic Association, i.e., prior to publication. Think of it as a &quot;reviewer&quot; of the data supplement. Analyze the provided materials - see Verification guidance Verify data citations Verify ability to post data (do the authors have the right to post the data?) If possible, attempt replication 4.1.2 Post-Publication Evaluation of Reproducibility We download articles and supplements, assess to what extent an undergraduate student can run the code that produces the analysis reported in the paper. We have also in the past done an evaluation of the response of authors when the code is only available &quot;upon request&quot;. Not currently an active project. This is a secondary goal, as time permits or as research goals suggest. It is quite similar to the first goal, but there is no interaction with authors, and no method to improve authors' files. Interested parties might visit ACRE for how to incorporate this kind of activity into a class curriculum. 4.2 Learning goals In this training, you will learn how to verify compliance with the policy. This means going through a variety of checklists, obtaining data, and running code, as per instructions provided by authors. You will not be required to actively program, but you will learn a lot about how to program (and sometimes, how not to program). You will gain an appreciation for a well-structured empirical analysis, which you will benefit from for your own studies (now) and in your work (after graduation). References "],["basic-concepts.html", "Chapter 5 Basic Concepts", " Chapter 5 Basic Concepts The goal thus of the reproducibility activity is to verify provenance of all data sources, to verify the availability of all computer code to produce analysis files from data sources, and to produce tables, figures, and in-text numbers from analysis files, to verify that the available code actually reproduces the analysis files, tables, figures, and in-text numbers. We therefore need to define a few concepts: what is a data source, and how can one verify its provenance? what is analysis data? how can we verify computational reproducibility? The subsequent chapters will explore each of these in additional detail. "],["data-citations-and-data-availability-statements.html", "Chapter 6 Data citations and data availability statements 6.1 A very short history of data citations 6.2 How do you create a data citation 6.3 Data availability statements", " Chapter 6 Data citations and data availability statements Most students and researchers have been trained to cite their sources. Mostly, this is meant to be literature sources, but the basic idea applies just as much to data: If you use somebody else's data, you should acknowledge that. 6.1 A very short history of data citations The almost complete absence of data citations from the literature has lead to issues when data creators and data providers attempt to assess their impact on science. Typically, they revert to manually or algorithmically scouring the literature, trying to find instances where their data is used. In 2016, a number of scientists and publishers from many domains got together and issued the FAIR Data principles (FORCE11 2016). &quot;FAIR&quot; is an acronym for Findable, Accessible, Interoperable, and Re-usable These are principles which help the furthering of increased reproducibility - by making data accessible, reproducibility checks can be conducted. By making them interoperable and re-usable, others can reproduce, replicate, and expand on the original research. Findability remains an issue: how do you discover data that is hidden in a ZIP file on a journal website as part of an (otherwise) perfectly reproducible package? In fact, the same group had previously laid the groundwork to that. In 2014, they issued the &quot;Data Citation Principles (DCP)&quot; (Martone 2014). These laid out the ethical need for such citations, the content such citations should have, and some desirable attributes of data citations. Both the DCP as well as FAIR suggest that data be assigned unique identifiers, allowing them to be clearly referenced, and found. The most common identifier in the social sciences is the Digital Object Identifier (DOI), but others exist, in particular in the biological or physical sciences. DCPrinciples The DCC note that (emphasis added): Sound, reproducible scholarship rests upon a foundation of robust, accessible data. For this to be so in practice as well as theory, data must be accorded due importance in the practice of scholarship and in the enduring scholarly record. In other words, data should be considered legitimate, citable products of research. Data citation, like the citation of other evidence and sources, is good research practice and is part of the scholarly ecosystem supporting data reuse. Data citation increases the findability, accessibility, interoperability, and re-usability of research data (FAIR). Through data citations, data providers can link to articles (sometimes automatically), allowing them to show the academic value of the data and continue providing the services around data creation. Finally, data citations open a new path to finding relevant science, by reaching the linked articles through data search interfaces, like openICPSR, Data-Pass, and Google Dataset Search. These principles are imperfectly implemented even today. Many data providers, including some of the biggest statistical agencies, do not provide unique identifiers to a particular data file. Some might uniquely identify a data series. Others may heuristically refer to certain versions (&quot;V2&quot;). Few have robust archival quality unique identifiers. Journals' stylistic considerations also reduce the impact of the DCC. Citations - in the sense of appearing in a bibliography at the end of the document - do not provide much sense of any access method other than a download. The AEA follows the Chicago Manual of Style (CMOS), with several additions on the AEA website. BLS citation at AEA As the CMOS states, one of the criteria for a useful citation is conveying authority and permanence: Electronic content presented without formal ties to a publisher or sponsoring body has the authority equivalent to that of unpublished or self-published material in other media. FAIR data helps convey such information, but often, assessing what &quot;publisher&quot; or &quot;sponsoring body&quot; is reputable or reliable is tricky. And the citation fails to convey many important facets of data access. What if the cited resource requires a login, even if it is free? What if payment is required? What if a long-winding application process is required? Citations do not communicate that amount of detail. As we shall see later, there are ways to augment data citations with the needed information. 6.2 How do you create a data citation ICPSR (ICPSR 2020) notes that a citation should include the following items: Author Title Distributor Date Version Persistent identifier where any data source has the first five elements, and the ideal FAIR curated data source also has a persistent identifier. 6.2.1 An imperfect real example Consider the BLS citation shown earlier: Bureau of Labor Statistics. 2000–2010. “Current Employment Statistics: Colorado, Total Nonfarm, Seasonally adjusted - SMS08000000000000001.” United States Department of Labor. http://data.bls.gov/cgi- bin/surveymost?sm+08 (accessed February 9, 2011). The author or creator is clearly Bureau of Labor Statistics. The distributor here is United States Department of Labor, which happens to be the government department housing the BLS. Title is arguably Current Employment Statistics: Colorado, Total Nonfarm, Seasonally adjusted, though some might argue that state, coverage, and seasonal adjustment identify versions of the survey. Date are the (original) dates of publication, or here, approximated by the date range covered by the series. But generally, versions relates to a chronologically released version - which is less clear in this case, and only captured by (accessed February 9, 2011). There is no clear persistent identifier, the closest approximation is provided by a combination of the series identifier SMS08000000000000001, the URL http://data.bls.gov/cgi-bin/surveymost?sm+08, and the date accessed (accessed February 9, 2011). Note that this is imperfect, because, unless you can time-travel, you cannot obtain the same dataset a second time. Attribute Value Author: Bureau of Labor Statistics Title: Current Employment Statistics: Colorado, Total Nonfarm, Seasonally adjusted Distributor: United States Department of Labor Date: 2000-2010 Version: (accessed February 9, 2011) Persistent identifier: SMS08000000000000001 + http://data.bls.gov/cgi-bin/surveymost?sm+08 + (accessed February 9, 2011) 6.2.2 An ideal (?) data citation What should the ideal data citation look like? ICPSR suggests Barnes, Samuel H. Italian Mass Election Survey, 1968. Ann Arbor, MI: Inter-university Consortium for Political and Social Research distributor, 1992-02-16. https://doi.org/10.3886/ICPSR07953.v1 so that Attribute Value Author: Barnes, Samuel H. Title: Italian Mass Election Survey, 1968 Distributor: Inter-university Consortium for Political and Social Research Date: 1992-02-16 Version: V1 Persistent identifier: 10.3886/ICPSR07953.v1 Note that the date 1968 describes the survey, but its date of publication was 1992. The version is implicit in the DOI. The persistent identifier is just 10.3886/ICPSR07953.v1, but the display guidelines for DOI suggest including the full URL that yields a resolution. 6.2.3 Data citation template In general, therefore, as long as you can fill out the table Attribute Value Author: Title: Distributor: Date: Version: Persistent identifier: you can create a data citation: [AUTHOR], &quot;[TITLE]&quot;, DISTRIBUTOR, [DATE], [VERSION] + [Persistent Identifier] 6.2.4 A not quite serious example Many authors initially neglect to add data citations, or do not know how to add a data citation. Often, we see authors cite papers with supplementary data, but not databases or other data: We use data acquired from the NHL, dates of power outages collected by Tremblay et al (2018), augmented with information on the language and grammar skills of hockey players provided by the Ethnologue database. (note absence of citation for NHL and Ethnologue data). In the above example, three datasets are used, but only one is cited in some fashion. 6.2.4.1 Better The above example can be improved as follows: We use data acquired from the NHL (NHL, 2018), dates of power outages collected by Tremblay et al (2018, 2019), augmented with information on the language and grammar skills of hockey players provided by the Ethnologue database (Eberhard et al, 2019). with the reference list having the following entries: Eberhard, David M., Gary F. Simons, and Charles D. Fennig (eds.). 2019. Ethnologue: Languages of the World. Twenty-second edition. Dallas, Texas: SIL International. Online version: http://www.ethnologue.com. National Hockey League. 2018. NHL Game Database 1917-2018. National Hockey League Hall of Fame, Toronto, ON. Accessed February 29, 2019. Tremblay, Réjean, Ken Dryden, and José Theodore. 2018. &quot;The impact of power outages on goal-keeping in the NHL&quot;, Journal of National Hockey Leagues, vol 32, iss. 1. Tremblay, Réjean, Ken Dryden, and José Theodore. 2019. &quot;Power outages during NHL games (updated)&quot;, Canadian Hockey Dataverse, doi:10.1234/nhl.lnh.haha Assess why the latter is better. 6.2.5 Data distributed as supplementary data The CMOS provides examples of how to cite supplementary materials that are attached to a specific article: Suárez-Rodríguez, M. and C. Macías Garcia. 2014. &quot;There Is No Such a Thing as a Free Cigarette: Lining Nests with Discarded Butts Brings Short-Term Benefits, but Causes Toxic Damage.&quot; Journal of Evolutionary Biology 27, no. 12 (December 2014): 2719–26, https://doi.org/10.1111/jeb.12531, data deposited at Dryad Digital Repository, https://doi.org/10.5061/dryad.4t5rt. The AEA guidance used to provide an example, in which the citation links to the article landing page: Romer, Christina D., and David H. Romer. 2010. “The Macroeconomic Effects of Tax Changes: Estimates Based on a New Measure of Fiscal Shocks: Dataset.” American Economic Review. https://doi.org/10.1257/aer.100.3.763. Many authors, however, would only cite the article itself, and not the data. Note however that modern data citation guidance suggest that both the article and the data used by the article should be cited, and this can lead to confusion. With the 2019 move of the AEA to a data archive, the correct citation for the above supplement would be: Romer, Christina D., and David H. Romer. 2010. &quot;Replication data for: The Macroeconomic Effects of Tax Changes: Estimates Based on a New Measure of Fiscal Shocks.&quot; American Economic Association [publisher], * Inter-university Consortium for Political and Social Research distributor*, https://doi.org/10.3886/E112357V1 with the article also cited as: Romer, Christina D., and David H. Romer. 2010. “The Macroeconomic Effects of Tax Changes: Estimates Based on a New Measure of Fiscal Shocks” American Economic Review. no. 3 (June 2010): 763–801. https://doi.org/10.1257/aer.100.3.763. 6.2.6 Producer Often, the creator of a dataset is an organization. The same way that an organization as a work's author can be cited: ISO (International Organization for Standardization). 1997. Information and Documentation—Rules for the Abbreviation of Title Words and Titles of Publications. ISO 4:1997. Paris: ISO. an organization can be cited as the creator of a dataset: Standard and Poor's (S&amp;P). 2017. Compustat-Capital IQ. S&amp;P Global Market Intelligence. 6.2.7 Distributor In many cases, the data are not distributed by the creator. This means the distributor takes on the role of a publisher (of a book, of data). In the BLS example, the two differed only because the (higher-ranking) department counts as the distributor. In the case of Compustat, one might have obtained access through the Wharton Research Data Services, and cite as Standard and Poor's (S&amp;P). 2017. Compustat-Capital IQ. Wharton Research Data Services. https://wrds-www.wharton.upenn.edu/pages/about/data-vendors/sp-global-market-intelligence/ If using the S&amp;P 500 data, there may be multiple providers: S&amp;P Dow Jones Indices LLC, S&amp;P 500 [SP500], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/SP500, January 24, 2020. S&amp;P Dow Jones Indices LLC, S&amp;P 500, provided via Haver Analytics Data Subscription, February 24, 2018. with hopefully the same content. Note that often, such data is subject to copyright and redistribution restrictions (see the page at FRED on SP500 and discussion in the later section). 6.2.8 Dates In some cases, it isn't clear when the dataset was published, though it may be clear what time period the dataset covers (as in the BLS case). One way to address this may be by using the &quot;n.d.&quot; abbreviation for the date of publication and including the date of coverage in the title: Standard and Poor's (S&amp;P). n.d. Compustat-Capital IQ (1982-2017). Wharton Research Data Services. Accessed April 6, 2018. https://wrds-www.wharton.upenn.edu/pages/about/data-vendors/sp-global-market-intelligence/ A related issue may arise when the dataset is comprised of multiple years, each of which has its own DOI. For instance, when accessing multiple years of American Community Survey data on ICPSR, each has its own DOI: American Community Survey (ACS): Public Use Microdata Sample (PUMS), 1998 (ICPSR 3888) 2008-05-21 American Community Survey (ACS): Public Use Microdata Sample (PUMS), 1997 (ICPSR 3886) 2008-05-21 American Community Survey (ACS): Public Use Microdata Sample (PUMS), 2003 (ICPSR 4117) 2009-12-01 American Community Survey (ACS): Public Use Microdata Sample (PUMS), 2004 (ICPSR 4370) 2008-10-14 American Community Survey (ACS): Public Use Microdata Sample (PUMS), 2009 (ICPSR 33802) 2013-04-04 American Community Survey (ACS): Public Use Microdata Sample (PUMS), 2008 (ICPSR 29263) 2011-11-08 One approach to this is to create a composite citation, with additional information available in an online data appendix or a Data Availability Statement: Bureau Of The Census. 2009. “American Community Survey (ACS): Public Use Microdata Sample (PUMS), 1997-2009.” United States Department Of Commerce [publisher]. ICPSR - Interuniversity Consortium for Political and Social Research. distributor DOIs listed in data appendix. or Bureau Of The Census. 2009. “American Community Survey (ACS): Public Use Microdata Sample (PUMS), 1997-2009.” United States Department Of Commerce [publisher]. ICPSR - Interuniversity Consortium for Political and Social Research. distributor https://www.icpsr.umich.edu/icpsrweb/ICPSR/search/studies?q=american+community+survey (accessed November 21, 2019) (and listing of exact DOIs in an appendix table). 6.2.9 Offline access mechanism Many datasets are available only under license, memorandum, contract, etc., and do not have a formal online presence. This is quite similar to traditional offline archives, for instance manuscript collections. For such collections, CMOS suggests: Kallen, Horace. Papers. YIVO Institute for Jewish Research, New York. Merriam, Charles E. Papers. Special Collections Research Center, box 26, folder 17. University of Chicago Library. and usage in the text as Alvin Johnson, in a memorandum prepared sometime in 1937 (Kallen Papers, file 36), observed that ... Similar citations can be constructed for offline databases: Bloom, Nick. 2019. Confidential survey data on Cameroon business processes. Stanford Secure Access Center (file &quot;cameroon-bloom.zip&quot;). Stanford University. 6.2.10 Confidential databases Similar forms may be used for confidential databases when no DOI exists: Internal Revenue Service. (YEAR). Corporate Income Tax Returns [database]. Department of Treasury, Washington DC, accessed YYYY-MM-DD. where the data, in this case, were accessed via the &quot;Department of Treasury,&quot; acting as a secure distributor (of access, not downloads). If the same data had been accessed via a secure research data center, the reference should have instead noted that access mechanism: Internal Revenue Service. (YEAR). Corporate Income Tax Returns [database]. Federal Research Data Centers, last accessed YYYY-MM-DD. 6.2.10.1 Confidential data with DOI If a DOI exists, of course, the formal citation generated from that DOI should be used: Forschungsdatenzentrum der Bundesagentur für Arbeit. 2020. “Betriebs-Historik-Panel (BHP) – Version 7518 v1.” Institut für Arbeitsmarkt- und Berufsforschung (IAB). https://doi.org/10.5164/IAB.BHP7518.DE.EN.V1. 6.2.11 No formal access mechanism In some cases (not infrequently), access to data is through informal means. The CMOS allows for citation of such information, without inclusion in the references. (A. P. Møller, unpublished data; C. R. Brown and M. B. Brown, unpublished data) We would deviate from that suggestion, ask for inclusion in the reference list, and simply suggest using unpublished data as the locator, similar to a URN, in the reference list: Møller, A. P. n.d. “Data on Crocodile Sightings in Manhattan.” unpublished data. Accessed February 29, 2019. 6.2.12 Unknown or confidential author In some cases, the authors might not be able to name the data creator, due to a non-disclosure agreement. One suggestion may then be Anonymous Firm, n.d. &quot;Data on financial transactions.&quot; Accessed under Non-disclosure Agreement, extract obtained on January 20, 2016. 6.2.13 Where to cite In all cases, data and code should be cited in the main manuscript. They should also be referenced in the data availability statement (some journals) or the README (other journals). However, in some cases, data is only used in an online appendix, and it is acceptable to cite the data there as well, and not in the main manuscript's bibliography. Furthermore, as data citation are still a relatively new concept, many authors will have substantially completed their manuscript, without including data citations. Adding them to the README is then acceptable practice (for now). 6.3 Data availability statements The academic publishing community's response are &quot;data availability statements (DAS).&quot; While mostly, these are pointers from the journal to where the data can be found. In the case of data supplements, this is almost trivial when the journal has a robust data availability policy, though some journals allow for self-declared but unverified DAS. Summarily, a data availability statement describes not just where the data can be obtained from, but also how the data can be obtained, conditions for obtaining it, and any additional restrictions. Some examples are provided by Springer/Nature and Hindawi: Springer Hindawi 6.3.1 Some examples of DAS 6.3.1.1 Example for public use data included in data archive: The paper uses data obtained from IPUMS (Ruggles et al, 2017). IPUMS-CPS does not currently provide the ability to store or reference custom extracts, but allows for redistribution for the purpose of replication. The archive contains the extracted data, codebook in the folder &quot;data/IPUMS&quot;. The data citation in the main article has the full URL. 6.3.1.2 Example for public use data not included in data archive: Data from the Socioeconomic High-resolution Rural Urban Geographic Dataset on India, Version 1.0 (Asher and Novosad, 2019) is used in this paper. The full dataset and documentation can be downloaded from https://doi.org/10.7910/DVN/DPESAK. 6.3.1.3 Example for public use data with required permission: The paper uses IPUMS Terra data. IPUMS-Terra does not allow for redistribution, except for the purpose of replication archives. Permissions as per https://terra.ipums.org/citation have been obtained, and are documented within the &quot;data/IPUMS-terra&quot; folder. 6.3.1.4 Example for confidential data: The data for this project are confidential, but may be obtained with Data Use Agreements with the Massachusetts Department of Elementary and Secondary Education (DESE). Researchers interested in access to the data may contact [NAME] at [EMAIL], also see www.doe.mass.edu/research/contact.html. It can take some months to negotiate data use agreements and gain access to the data. The author will assist with any reasonable replication attempts for two years following publication. 6.3.1.5 Example for Government registers In some cases, governments have a list of their (named) registers. For instance, Statistics Denmark provides the full list of registers at http://www.dst.dk/extranet/forskningvariabellister/Oversigt%20over%20registre.html. These can be used to craft data citations (see Data citation guidance). Data availability statements should describe how each such register can be accessed: The information used in the analysis combines several Danish administrative registers (as described in the paper). The data use is subject to the European Union’s General Data Protection Regulation(GDPR) per new Danish regulations from May 2018. The data are physically stored on computers at Statistics Denmark and, due to security considerations, the data may not be transferred to computers outside Statistics Denmark. Researchers interested in obtaining access to the register data employed in this paper are required to submit a written application to gain approval from Statistics Denmark. The application must include a detailed description of the proposed project, its purpose, and its social contribution, as well as a description of the required datasets, variables, and analysis population. Applications can be submitted by researchers who are affiliated with Danish institutions accepted by Statistics Denmark, or by researchers outside of Denmark who collaborate with researchers affiliated with these institutions. (Example taken from Fadlon and Nielsen, 2020 (forthcoming as of June 2020). 6.3.1.6 S&amp;P 500 The S&amp;P 500 is one of the most widely known stock indexes. And yet, it is subject to copyright, and restrictions on redistribution. Some authors who use the S&amp;P 500 numbers may have downloaded it via FRED https://fred.stlouisfed.org/series/SP500, others through other data services (Haver Analytics, Bloomberg). The FRED website mentions that the data are S&amp;P Dow Jones Indices LLC. Reproduction of S&amp;P 500 in any form is prohibited except with the prior written permission of S&amp;P Dow Jones Indices LLC (&quot;S&amp;P&quot;). If obtained through FRED, the suggested citation is S&amp;P Dow Jones Indices LLC, S&amp;P 500 [SP500], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/SP500, January 24, 2020. An analogue if accessing it through, say, Haver Analytics, might be S&amp;P Dow Jones Indices LLC, S&amp;P 500, provided via Haver Analytics, January 24, 2020. Note that both citations do not provide (complete) information on how others might obtain the data, and what restrictions are imposed on the data (unless they visit the site, or read the terms of use of, say, their Haver Analytics description). A tentative Data Availability Statement might be: S&amp;P 500 data is (C) S&amp;P Dow Jones Indices LLC. Reproduction of S&amp;P 500 in any form is prohibited except with the prior written permission of S&amp;P Dow Jones Indices LLC (&quot;S&amp;P&quot;). It is thus not available as part of the replication archive. Users may access the past 10 years via FRED at https://fred.stlouisfed.org/series/SP500, or purchase longer access via Haver Analytics (http://www.haver.com/databaseprofiles.html#indicators). Note that reproduction of our manuscript's tables requires data from [YYYY]-[ZZZZ]. References "],["generic-data-workflow.html", "Chapter 7 Generic data workflow", " Chapter 7 Generic data workflow graph TD; subgraph Dataflow; A((Input data)) ==&gt; B[Cleaning programs]; B ==&gt; C((Analysis data)); C ==&gt; D[Analysis programs] D ==&gt; E((Outputs)); end; B -.-&gt; F((&quot;Auxiliary data(created)&quot;)); F -.-&gt; C; Z((Source)) -.-&gt; X[Data citation] -.-&gt; A; "],["assessing-computational-reproducibility.html", "Chapter 8 Assessing computational reproducibility", " Chapter 8 Assessing computational reproducibility "],["privacy.html", "A Privacy A.1 Privacy of Replicators A.2 The Privacy of Authors", " A Privacy We need to cover two sorts of privacy: the privacy of those whose materials we verify, and your own privacy. There are limitations to both, but we attempt to protect privacy as much as possible. A.1 Privacy of Replicators You are tasked with reproducing articles. Much as referees for journals mostly remain anonymous, we want you to remain anonymous as well. You may reveal yourself to authors later (after the task is completed), if you wish. You should not contact authors unless authorized by the Lab Leader. Normally, all such communications go through the Lab Leader. We do name you (to thank you) in the annual report, but do not attribute your work to any one article. In the empirical analysis of all the articles, we replace your netid and name with an anonymous (and untraceable) identifier. So we can track that you have done Articles A1, D57, and Z31, but nobody knows that it was you. There is &quot;leakage&quot; of information: In order to download materials, you need to login to openICPSR, and have the ability to download from specific deposits. This does reveal your name to the depositors. This is currently a technological constraint, and cannot be avoided without great complications. If you have concerns, please let us know, and we will find a workaround. Should you ever be contacted in some unacceptable fashion by authors, you should immediately contact the Lab Leadership. You can, and you should, reveal your affiliation with this project! You can (and you should) be proud of the work you will do or have done, and you are allowed (and you should) reference this project as an accomplishment. A.2 The Privacy of Authors When we do pre-publication verification, this is equally important. You are never allowed to reveal that the author has submitted to the journal This includes when you need to contact third parties for materials that are part of the replication materials. In case of doubt, contact Lab Leadership. You are never allowed to reveal anything about the analysis that the author is conducting, and that you are reproducing, to anybody outside of this group. You must never put the code, the article, or the data on a location where others outside of this group could access it Bitbucket within the aeaverification project is OK, do not attempt to make a repository public (even if it may seem convenient not to have to enter your login etc.) Remove the files from your laptop as soon as you are done with it (after git push, of course) You may remove them from CISER nodes, but those will be cleansed later Do not email or otherwise disseminate (twitter, facebook, snapchat, whatever) the files received, or any other information about the papers "],["checklist.html", "B Setup Checklist B.1 Accounts you will need to sign up for (action required) B.2 Accounts you will be signed up for (no action required) B.3 Software to install on your laptop B.4 Text editor vs. Word processor B.5 Availability and Suggestions B.6 Note B.7 Help", " B Setup Checklist B.1 Accounts you will need to sign up for (action required) [ ] CISER account (for computing access) - Select &quot;Apply for Research account&quot; and list Lars as a sponsor. Do not select &quot;Request Secure Data Services account&quot;. If prompted about using sensitive or restricted data, please select &quot;No&quot;. Otherwise, this will take you to a window to sign up for a CRADC account instead, which is not necessary. If you encounter this issue, please email the CISER help desk asking them cancel the CRADC account request. If using a Mac and if &quot;Microsoft Remote Desktop&quot; is not already installed on Mac, install Remote Desktop Software from iTunes (not necessary on Windows, as it is pre-installed) Once an account is set up, learn How to log on to CISER (also includes download links) [ ] You will need openICPSR account, in order to download pre-publication materials. Please be sure to use your Cornell e-mail! B.2 Accounts you will be signed up for (no action required) [ ] Atlassian account Bitbucket for access to the internal Git repos Jira account for the internal issue tracker [x] Email is used for the mailing list ldi-lab-l@cornell.edu B.3 Software to install on your laptop [x] Command line (PowerShell, or Terminal, your choice) - pre-installed [ ] Windows Remote Desktop (see above) [ ] Git command line tool (download the software, then follow the guide on Installing Git on your computer [ ] Visual Studio Code (download location), a powerful text editor [ ] Sourcetree (Git graphical interface optimized for Bitbucket) download location (OPTIONAL) Other software is optional. You will use statistical software on other computers that we will get you access to. B.4 Text editor vs. Word processor You want to use a text editor, not a word processor. The difference: a text editor creates simple text files, without fancy formatting. If you are creating code in Stata, Matlab, or Rstudio, you are using a customized text editor, sometimes called an IDE = &quot;Integrated Development Environment&quot;. That is OK. But remember that all such program code is a straight text file. General purpose text editors can view and edit them all, although they may lack some of the fancy features that make programming easier in the dedicated IDE. B.5 Availability and Suggestions OS Laptop CISER Custom node MS Visual Studio Code All Suggested Yes Atom All Yes Notepad++ Windows Yes Yes vi Mac, Linux B.6 Note Our preferred editor (Visual Studio Code) is not available on CISER nodes. However, it is possible to install, though a bit tricky (advanced). B.7 Help Git cheatsheet and another one Markdown cheatsheet "]]
